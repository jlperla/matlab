\documentclass[nofootline]{etk-presentation}
\usepackage{pstool}
\mintedwithaux %Loads the \usepackage{minted} with options for the auxilary files being moved.

\begin{document}
\title{Optimization, Auto-Differentiation, and Tomlab}
%\author[Jesse Perla]{\large Jesse Perla \\ {\small \textit{University of British Columbia}}}
%	\date{May 2017}
%\date{} %Shows date and conditionally the revision.	
\maketitle
	\begin{frame}\frametitle{}
	\bigskip
	\bigskip
	\bigskip	
	\begin{center}
		{\huge \emphcolor{Auto-Differentiation(AD) }}
	\end{center}
\end{frame}
\begin{frame}[fragile]	\frametitle{Derivatives and Numerical Methods}
	There are two general types of algorithms for optimizers/solvers/etc.:
	\begin{enumerate}
		\item Derivative-free:
		\begin{itemize}
			\item e.g. Simplex and Nelder-Mead.  This is Matlab's \verb|fminsearch|
			\item Also, ``costly global function'' optimization
			\item Avoid at all costs (though sometimes don't have a choice)
		\end{itemize}
	\bigskip
		\item Derivatives
		\begin{itemize}
			\item Pretty much every other algorithm, especially for large number of variables/constraints
			\item Including global optimization techniques (which use derivatives locally)
		\end{itemize}	
	\end{enumerate}
\bigskip
Key derivatives to calculate are:
\begin{itemize}
	\item Gradient of objective
	\item Hessian of objective (nonlinear least squares and some algorithms only use gradient)
	\item Jacobian of constraints
\end{itemize}


\end{frame}

\begin{frame}[fragile]	\frametitle{Calculating Derivatives}
How to calculate derivatives for the objective and constraints?
\begin{enumerate}
	\item Calculate by hand
	\begin{itemize}
		\item Sometimes, though not always, the most accurate and fastest option
		\item But algebra is error prone for non-trivial setups
		\item (note: many optimizers have a way to check your analytical derivatives)
	\end{itemize}
\bigskip
	\item Finite-differences:
	\begin{itemize}
		\item $\D[x_i]f(x_1,\ldots x_N) \approx \frac{f(x_1,\ldots x_i + \Delta,\ldots x_N) - f(x_1,\ldots x_i,\ldots x_N)}{\Delta}$
		\item Evaluates function at least $N$ extra times to get a gradient
		\item \# evaluations for Jacobians with $M$ constraints even worse
		\item Large $\Delta$ is numerically stable but inaccurate, small $\Delta$ is unstable
		\item \textbf{Avoid like the plague!} (and is what matlab does out of the box)
	\end{itemize}
\bigskip
	\item Auto-differentiation
	\begin{itemize}
		\item Not a form of finite-differences or numeric differentiation
		\item Essentially analytical.  Repeated use of the chain-rule
		\item Does not work for every function, but only evaluates $f(\cdot)$ once if it works---i.e. $O(1)$ not $O(N\times M)$ for $f : \R^N \to \R^M$
	\end{itemize}
\end{enumerate}
\end{frame}


\begin{frame}[fragile]	\frametitle{Auto-differentiation (adapted from Wikipedia)}
	\begin{itemize}
		\item Remember the chain rule: $\frac{dy}{dx} = \frac{dy}{dw} \frac{dw}{dx}$
		\item Consider functions composed of calculations with fundamental operations (with known analytical derivatives)
		\item For example, consider function: $f(x_1, x_2) = x_1 x_2 + \sin(x_1)$	
	$$
\begin{array}{l|l}
\text{Operations to compute value} &
\text{Operations to compute $\frac{d f(x_1,x_2)}{d x_1}$}
\\
\hline
w_1 = x_1 &
\frac{d w_1}{d x_1} = 1 \text{ (seed)}\\
w_2 = x_2 &
\frac{d  w_2}{d x_1} = 0 \text{ (seed)}
\\
w_3 = w_1 \cdot w_2 &
\frac{d  w_3}{d x_1} = w_2 \cdot \frac{d  w_1}{d x_1} + w_1 \cdot \frac{d  w_2}{d x_1}
\\
w_4 = \sin w_1 &
\frac{d  w_4}{d x_1} = \cos w_1 \cdot \frac{d  w_1}{d x_1}
\\
w_5 = w_3 + w_4 &
\frac{d  w_5}{d x_1} = \frac{d  w_3}{d x_1} + \frac{d  w_4}{d x_1}
\end{array}
$$
\item Generalizes to multiple variables.  AD takes source code and generates the derivatives at the same time (i.e. doesn't increase with \# variables)
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{Implementations of AD}
	\begin{itemize}
		\item A field unto itself.  Do not implement directly
		\item Implementation is language dependent.  Two approaches:
		\begin{itemize}
			\item \textit{Source code transformation:} utility (outside of the language itself) reads in the code for your function, and generates a function which calculates value and derivative.  Rerun if you change your code
			\item \textit{Operator Overloading:} Takes your existing functions, and passes variables that act like numbers, but are actually recording and tracing the chain rule steps/etc.  Can be magical, or infuriating
		\end{itemize}
		\item Implementation depends on the language:
		\begin{itemize}
			\item \textit{Fortran:} usually needs SCT.  Many choices: e.g. \url{http://tapenade.inria.fr:8080/tapenade/index.jsp}
			\item \textit{Python:} \url{https://github.com/LowinData/pyautodiff} and \url{https://pythonhosted.org/algopy/}
			\item \textit{C++:} overloading \url{http://www.fadbad.com/fadbad.html},\ldots
			\item \textit{R:} \url{https://cran.r-project.org/web/packages/madness/index.html}
			\item \textit{Matlab:} open source SCT (e.g. AdiMat) not very good.  Use Tomlab/MAD instead, coupled with the Tomlab optimizer.
		\end{itemize}
	\end{itemize}
\end{frame}

	\begin{frame}\frametitle{}
	\bigskip
	\bigskip
	\bigskip	
	\begin{center}
		{\huge \emphcolor{Sparsity}}
	\end{center}
\end{frame}

\begin{frame}[fragile]	\frametitle{Sparse Matrices and Methods}
	\begin{itemize}
		\item Many algorithms are specialized for matrices (or Jacobians or Hessians) with many $0$s---e.g. Gaussian elimination
		\item  Only store non-zero values, but $0 \neq 0.0$ for optimizers
		\item Not (usually) for storage, but rather specialized algorithms
		\item For Jacobians and Hessians, can solve enormous (e.g. hundreds of thousands or millions) of variable systems
		\begin{itemize}
			\item But the more non-zeros, the more likely dense methods are preferable.
		\end{itemize}

				\item For example, $f: \R^N \to \R^N$ with $f(x) = \sqrt{x}$ point-wise
				\begin{itemize}
					\item Jacobian has $N$ non-zeros, while dense has $N^2$
					\item Optimizers/solvers can use this to step in the right direction
					\item Auto-differentiation will figure out the sparsity pattern of derivatives---i.e., which values are always $0$ for all inputs
				\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}[fragile] \frametitle{Sparse Matrices in Matlab}
	\begin{minted}[fontsize=\footnotesize]{matlab}
%First, can convert dense matrix, and it drops the 0's.
X = [1.0 0   0;
     2.0 1.0 0];
S = sparse(X)
%S =
%(1,1)        1
%(2,1)        2
%(2,2)        1

%Or can take lists of indices and values,
x_indices = [1; 2; 2];
y_indices = [1; 1; 2];
values = [1; 2; 1];
S2 = sparse(x_indices, y_indices, values)

%Or can preallocate and just reference in loops/etc.
S3 = sparse(0,3);
S3(1,1) = 1;
S3(2,1) = 2;
S3(2,2) = 1;
	\end{minted}
\end{frame}

\begin{frame}\frametitle{}
	\bigskip
	\bigskip
	\bigskip	
	\begin{center}
		{\huge \emphcolor{Tomlab}}
	\end{center}
\end{frame}

\begin{frame}[fragile]	\frametitle{What is in Tomlab?}
	\begin{itemize}
		\item Sadly, the Operations Research community keeps the best implementations closed-source
		\item Collection of sparse/dense linear/nonlinear local/global constrained/unconstrained continuous/mixed-integer optimizers
		\item Nonlinear methods have built in auto-differentiation
		\item Repackages and resells state-of-the-art commercial products, and adds a few of its own which are high quality		
		\item Several methods to solve the same type of problem, because you never know which one will work best.  Easy to swap
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{What Types of Problems?}
	See \url{http://tomopt.com/docs/TOMLAB_QUICKGUIDE.pdf}. 
	\begin{itemize}
		\item Programming = Optimizer in OR
		\item Linear Programming (LP) and Mixed-Integer LP (MILP)
		\item Constrained Nonlinear Programming (NLP)
		\item Unconstrained Global Optimization (glb)
		\item Linear Least Squares (LLS)
		\item Nonlinear Least Squares (NLLS)
		\item Solving systems of equations generally uses NLLS
		\item \ldots and many others (semi-definite, quadratic, etc.)
	\end{itemize}
\medskip
	Most have sparse vs. dense algorithms, and constrained vs. unconstrained
	\begin{itemize}
		\item Read docs to find best fit for your particular problem
		\item Always use appropriate constraints (none, box-bounded, linear, etc.)
		\item For borderline sparse problems, sometimes dense works better
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{Purchased Packages}
	See \url{http://tomopt.com/tomlab/products/}
	\begin{itemize}
		\item Stanford 
		Systems Optimization Laboratory (SOL):\\SNOPT, NPSOL, NLSSOL, LSSOL\ldots
		\item Knitro.  Good for big problems, and complementarity conditions
		\item MAD (auto-differentiation)
		\item LGO and CGO (global optimizers, costly and otherwise)
		\item For a given problem type, tomlab will list available algorithms
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{Setting up Tomlab}
	\begin{itemize}
		\item Use the UBC Matlab license
		\begin{itemize}
			\item If you aren't sure what you have, open matlab, type 'license' and check the nubmer is '924490'.  Otherwise, follow department/ubc instructions
		\end{itemize}
		\item Download Tomlab from the site or 
		\begin{itemize}
			\item \url{http://jesseperla.com/tomlab/tomlab-win64-setup.exe }
			\item or \url{http://jesseperla.com/tomlab/tomlab-osx64-setup.dmg}
		\end{itemize}
		\item Run to install it
		\item Put \url{http://jesseperla.com/tomlab/tomlab.lic} into main tomlab directory (with the startup.m file)
		\item You now have two choices:
		\begin{itemize}
			\item Whenever you want to use it, run the \verb!startup.m! script in tomlab
			\item Or, put the tomlab directory in your matlab path (with "Set Path" menu) and it runs on its own when starting matlab
		\end{itemize}
	\item Finally, Tomlab replaces the \verb!fmincon!, \verb!fsolve! etc. in matlab.  Avoid by delete/rename in the \verb!/optim! folder in installation
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{Linear Least Squares (i.e. Regression)}

	\begin{align*}
	\min_x &\frac{1}{2}\norm{C x - d}_2\\
	\st & x_L \leq x \leq x_U\\
	& b_L \leq A x \leq b_U
	\end{align*}
	
	\begin{itemize}
		\item Little benefit over stata until problems get large or sparse
		\item Though if there are linear constraints, $A$, may be helpful
		\item Major benefit for large, sparse $C$
		\item See ``Section 11. LLS Problem'' in \url{http://tomopt.com/docs/TOMLAB_QUICKGUIDE.pdf}
	\end{itemize}
\end{frame}


\begin{frame}[fragile]	\frametitle{Example: Two-way fixed Effect}
	\begin{itemize}
		\item 	Use student, $i$, and instructor, $j$ fixed effects with observables
			$$
		\text{grade}_{ij} = \text{observables}_{ij} + \text{student}_i + \text{instuctor}_j + \epsilon_{ij}
		$$
		\item $300$K observations for $36$K students and $945$ instructors
		\begin{itemize}
			\item $\approx 37$K variables, \textbf{but only $2$ non-zero} (plus observables)
		\end{itemize}
		\item Took about a day to solve in Stata
		\item See \verb!teacher_student_fixed_effect.m! example for generating sparse matrix.  Given \verb!id1! student id, and \verb!id1! instructor id.  Key code:
	\end{itemize}
\begin{minted}[fontsize=\footnotesize]{matlab}
%Preallocate a sparse matrix
%Total number of observables with indicators for the two types.
X = sparse(N_observations, N_observables + N_students + N_teachers); 

%Filling in indicators for the matches
for i=1:N_observations
X(i, N_observables + id1(i)) = 1; %set student indicator
X(i, N_observables + N_students + id2(i)) = 1; %sets instructor indicator
end
\end{minted}
\end{frame}

\begin{frame}[fragile]	\frametitle{Solving LLS in Tomlab}
	\begin{itemize}
		\item Given $X$ and $y$ such that $\min_\beta\norm{X \beta - y}_2$
		\item Ensure $X$ loaded sparse, with each row having 2 indicators
	\end{itemize}
\begin{minted}[fontsize=\footnotesize]{matlab}
%linear least squares, can pass in sparse matrices or use dense
help llsAssign %Can see options, if you wish.  Or use manual

% tomlab convention, call XXXAssign for problem type XXX
Prob = llsAssign(X, y, [], [], 'LLS Example'); 

%Can change settings.  See tomlab documentation
Prob.optParam.MaxIter = 5000; %optional, increase iterations
Prob.PriLevOpt  = 1; %optional, gives more information if higher

%Tomlab convention: tomRun, passing in the algorithm type and problem
%Takes about 10-20 seconds to run, instead of a day.  Not even tweaked
Result = tomRun('Tlsqr', Prob); %intended for sparse unconstrained LLS
beta = Result.x_k;

%Tried alternative methods.  Easy to swap
%Result = tomRun('snopt', Prob); %Tlsqr works much better here
\end{minted}

\end{frame}
	
\begin{frame}\frametitle{}
	\bigskip
	\bigskip
	\bigskip	
	\begin{center}
		{\huge \emphcolor{Constrained Optimization }}
	\end{center}
\end{frame}

\begin{frame}[fragile]	\frametitle{Nonlinear Programming (NLP)}
Given $f : \R^N \to \R$ and $c : \R^N \to \R^M$
	\begin{align*}
		\min_x &\set{f(x)}\\
		\st & x_L \leq x \leq x_U\\
		& b_L \leq A x \leq b_U\\
		& c_L \leq c(x) \leq c_U
	\end{align*}
	
	\begin{itemize}
		\item See ``Section 7. NLP Problem'' in \url{http://tomopt.com/docs/TOMLAB_QUICKGUIDE.pdf}
		\item Can solve large problems in general with dense solvers (e.g. tens, hundreds, thousands of variables depending on structure)
		\item Can solve enormous problems if Hessian of $f(x)$, Jacobian of $c(x)$ sparse (e.g. hundreds of thousands or millions of variables)
		\item If constraints are bounds, use $x_L, x_U$.  If linear, use $A$.  Equality set $b_L(m) = b_U(m)$ where appropriate.  No bounds, leave unconstrained 
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{Example: Porfolio Choice under Rational Inattention}
	\begin{itemize}
		\item Entropy constraint, $\kappa$ and signal variances, $\sigma_n$, weights $\alpha_n$
		\item Choose precision $x_n$ 
\begin{align*}
\min_{\set{x_n}}&\sum_{n=1}^N \alpha_n^2 x_n^2\\
\st & \frac{1}{2}\sum_{n=1}^{N}\left(\log\sigma^2_n -  \log x^2_n\right) \leq \kappa\\
& 0 < x_n < \infinity
\end{align*}
		\item Generate some $\set{\alpha_n,\sigma_n}$ and solve
		\item How large for $N$?  Using \verb!fminsearch! naively, got to $N\approx 30$.  Sample code I give you solves with $N = 100,000$ in $\approx$ 20 seconds
		\item Since the hessian is sparse, could potentially use specialized methods (but didn't even bother to play with that, just using AD)
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{Solving in Tomlab (with Anonymous Functions)}
\begin{minted}[fontsize=\footnotesize]{matlab}
%...define kappa, alpha vector of length N, sigma vector of length n...
objective = @(x) sum((x.^2).*(alpha.^2)); %Objective
constraint = @(x) (1/2)*sum( log(sigma.^2) - log(x.^2) ); %Constraint

%Bounds on x, and initial guess
x_L = 1E-10 * ones(N,1); %Bounding a little above 0 since it takes logs.
x_U = inf(N,1); %Upper bounds for x.
x_0 = 1.1*ones(N,1); %Some initial conditions

%Generate problem.  Use `help conAssign' to see arguments, or use manual
madinitglobals; %Need to run for auto-differentiation to work.
Prob = conAssign(objective, [], [], [], x_L, x_U, 'Portfolio example',...
   x_0, [], [], [], [], [], constraint, [], [], [], kappa, kappa);
Prob.ADObj = 1; % Gradient with AD. ADObj = -1 for Hessian
Prob.ADCons = 1; % Jacobian with AD. ADCons = -1 for constraint Lagrangian 

%Run type
Result = tomRun('knitro', Prob); %Choose algorithm. See tomlab for options
x_optimal = Result.x_k;

%Result = tomRun('npsol', Prob, 1) %Could try another algorithm

\end{minted}
\end{frame}

\begin{frame}[fragile]	\frametitle{Using External Functions and Fixed Parameters}

\begin{minted}[fontsize=\footnotesize]{matlab}
%Alaternatively, the objective/constraint can be in separate files
%Assumption is that vectors alpha/sigma/kappa attached to problem

%File: portfolio_objective.m
function f = portfolio_objective(x, Prob)
  f = sum((x.^2).*(Prob.alpha .^2));
end

%File: portfolio_constraint.m
function c = portfolio_constraint(x, Prob)
  c = (1/2)*sum( log(Prob.sigma.^2) - log(x.^2) ) - Prob.kappa;
end
\end{minted}	
\end{frame}	

\begin{frame}[fragile]	\frametitle{Calling with External Functions}
\begin{minted}[fontsize=\footnotesize]{matlab}
%Changed to have c(x) = 0 for the constraint since in c(x)
Prob = conAssign(@portfolio_objective, [], [], [], x_L, x_U, 'Example',...
 x_0, [], [], [], [], [], @portfolio_constraint, [], [], [], 0, 0);

%Put any constants into the Prob, available in the function
%Can throw anything you want only Prob (e.g. vectors, cells, etc.)
Prob.alpha = alpha;
Prob.sigma = sigma;
Prob.kappa = kappa;

%Setup AD
Prob.ADObj = 1; % Gradient with AD. ADObj = -1 for Hessian
Prob.ADCons = 1; % Jacobian with AD. ADCons = -1 for constraint Lagrangian 

%Run the optimizer
Result = tomRun('knitro', Prob); %The last 	
\end{minted}	
\end{frame}	
	
\begin{frame}\frametitle{}
	\bigskip
	\bigskip
	\bigskip	
	\begin{center}
		{\huge \emphcolor{Systems of Equations and NLLS}}
	\end{center}
\end{frame}

\begin{frame}[fragile]	\frametitle{Nonlinear Least Squares (NLLS)}
	Given residual $r : \R^N \to \R^M$ and $c : \R^N \to \R^P$
	\begin{align*}
		\min_x &\set{\frac{1}{2}r(x)^T r(x)}\\
		\st & x_L \leq x \leq x_U\\
		& b_L \leq A x \leq b_U\\
		& c_L \leq c(x) \leq c_U
	\end{align*}
	
	Also for solving system of equations (potentially with inequalities):
	\begin{align*}
		r(x) = \mathbf{0}
	\end{align*}
	\begin{itemize}
		\item See ``Section 13. NLLS Problem'' in \url{http://tomopt.com/docs/TOMLAB_QUICKGUIDE.pdf}
		\item Can solve very large problems, with/without constraints
%		\item No Hessian information is used in algorithms, so just gradient/jacobian.
%		\item If constraints are bounds, use $x_L, x_U$.  If linear, use $A$.  Equality set $b_L(m) = b_U(m)$ where appropriate.  No bounds, leave unconstrained 
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{Solving NLLS}
	\begin{minted}[fontsize=\footnotesize]{matlab}
%Residual
r = @(x) x.^2 - [2; 1];	
x_0 = [5; 10];
%Create object
Prob = clsAssign(r, [], [], [], [], 'NLLS example', x_0, zeros(2,1),[]);
Prob.ADObj = 1; % Use AD

%Solve it.
Result = tomRun('nlssol', Prob, 1);
	\end{minted}
\end{frame}	
	

\begin{frame}[fragile]	\frametitle{Solving Functional Equations}
	\begin{itemize}
\item	One important use of this is solving functional equations of the form
	$$\Phi(f) = \mathbf{0}$$
	\begin{itemize}
		\item Where $f : \R^N \to \R^M$ and $\Phi$ is an operator $\Phi : \C(\R^N) \to \R^P$
		\item Could include differential equations, difference equations, etc.
	\end{itemize}
\item To solve, can approximate $f$ with a basis.  Collocation methods, etc.
\begin{itemize}
	\item e.g. $f(x) \approx \sum_{q=1}^{Q}d_q P_q(x)$
	\item Where $P_q(x)$ is a polynomial/spline/finite-element basis
	\item $d_q$ are the unknown coefficients to solve for
\end{itemize}
\item Then, problem is to find the coefficients
$$
\min_{d} \set{\frac{1}{2}\Phi(d)^T \Phi(d)}
$$
\item Since for fixed $x_n$ nodes, can usually evaluate $P_q$ through linear algebra can use AD on $\Phi(d)$
\item Note: if $\Phi(d)$ is a linear operator (e.g. linear PDEs) can use sparse LLS.  How huge numerical PDEs with millions of points are solved.
\end{itemize}
\end{frame}		

\begin{frame}\frametitle{}
	\bigskip
	\bigskip
	\bigskip	
	\begin{center}
		{\huge \emphcolor{AD with Tomlab }}
	\end{center}
 \end{frame}

\begin{frame}[fragile] \frametitle{Using AD Directly in Tomlab}
	\begin{itemize}
	\item Keep in mind that optimizers/solvers in Tomlab do this automatically
	\item But if having trouble with optimizer calls, can test function separately
	\end{itemize}
\begin{minted}[fontsize=\footnotesize]{matlab}
%Example function.  Also works fine with separate files/function defs
f = @(x) 3*x + exp(x);

%Evaluating function
x_val = 2.1;
f(x_val)

%Evaluating with derivative at x_val
x = fmad(x_val, 1); %Seed, since dx/dx = 1
f_val = f(x)

%Extract (both calculated at same time)
getvalue(f_val)
getderivs(f_val)
\end{minted}

\end{frame}

\begin{frame}[fragile]	\frametitle{Black Magic? Is it Always so Easy?}
	\begin{itemize}
		\item Auto-differentiation works seamlessly for functions composed of an arbitrarily complicated graph of simple functions
		\begin{itemize}
			\item Just need analytical derivatives for the lowest-level functions
			\item Functions of vector and matrices are no problem at all.  In fact, the field was designed for large numbers of variables/constraints and sparsity
		\end{itemize}
	\bigskip
		\item Can you call other functions (with operator overloading)?
		\begin{itemize}
			\item Depends on how they were written.  Often no problem at all
			\item If the functions assume arguments are numbers, there can be problems
			\item Sometimes can fix the underlying code to make more generic (example)
		\end{itemize}
	\bigskip
		\item Verboten: Iterations and fixed-points \textit{within a function}
		\begin{itemize}
			\item e.g. it can't differentiate an optimization step within a function
			\item However, many algorithms can be re-written without nesting (e.g. nested fixed-point vs. MPEC for discrete-choice estimation)
			\item Possible that simulation could be embedded (e.g. mixed-logit)\ldots
		\end{itemize}
	\end{itemize}
\end{frame}	

\begin{frame}[fragile]	\frametitle{Keep Functions Generic}
	\begin{itemize}
		\item Remember, MAD replaces arguments with things that look like variables.  Keep everything generic, don't overwrite with other types
		\item Some internal matlab functions do this sort of thing
		\item Sometimes can copy/paste others sourcecode and tweak
	\end{itemize}

\begin{minted}[fontsize=\footnotesize]{matlab}
madinitglobals; %Need to run for auto-differentiation to work
x_val = [2.1;3.0];
x = fmad(x_val, eye(2,2)); %Seeds with derivatives

%Extract (both calculated at same time)
f_val = f_func(x)

function y = f_func(x)
  y = x.^2; %This leaves x, y generic
  %x = 1; %Don't do this!!!!!!
  %y = zeros(1,1) %Don't do this!!!!!
  %y(1) = x.^2; %indexing is fine (as long as you do not preallocate)
  %One trick is to allocate as something like: y = x, then index
end
\end{minted}	

\end{frame}

\begin{frame}[fragile]	\frametitle{Missing Function (with Analytical Derivative)}
\begin{itemize}
	\item See MAD manual, \url{http://tomopt.com/docs/TOMLAB_MAD.pdf}
	\item Section: \verb!"Adding Functions to the fmad Class"!
	\item Example, \verb!normcdf! isn't there, could add something like (untested):
\end{itemize}

\begin{minted}[fontsize=\footnotesize]{matlab}
%Add to the appropriate location in tomlab
%This should work for vectors/matrices since using .*
function y=normcdf(x)
	y = x; %Needs to copy		
	y.value= normcdf(x.value); %Evaluate given double 
	y.deriv = normpdf(x.value) .* x.deriv; %Note chain rule
end
\end{minted}
\end{frame}



\end{document}